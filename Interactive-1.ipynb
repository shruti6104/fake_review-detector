{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to Python 3.10.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ada6ec-cced-4efc-a8e5-b77ff15b5f80",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\AKANKSHA\\OneDrive\\Desktop\\fake review detect\\data\\main.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mjoblib\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mplt\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mwordcloud\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m WordCloud\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m stopwords\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstem\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m WordNetLemmatizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "import os\n",
    "import string\n",
    "import nltk\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# === NLTK Downloads ===\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# === Step 1: Load Dataset ===\n",
    "df = pd.read_csv(\"reviews.csv\")\n",
    "print(\"üìã Column names:\", df.columns)\n",
    "\n",
    "# === Step 2: Rename Columns if Needed ===\n",
    "if 'text_' in df.columns:\n",
    "    df.rename(columns={'text_': 'review'}, inplace=True)\n",
    "if 'label' not in df.columns:\n",
    "    raise ValueError(\"‚ùå 'label' column missing in dataset\")\n",
    "\n",
    "# === Step 3: Clean the Text ===\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['cleaned_review'] = df['review'].apply(clean_text)\n",
    "print(\"\\nüîç Cleaned Reviews:\\n\")\n",
    "print(df[['review', 'cleaned_review']].head())\n",
    "\n",
    "# === Step 4: TF-IDF Vectorization ===\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(df['cleaned_review'])\n",
    "y = df['label']\n",
    "\n",
    "# === Step 5: Train/Test Split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# === Step 6: Model Training ===\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# === Step 7: Evaluation ===\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"\\nüìä Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# === Step 8: Save Model and Vectorizer ===\n",
    "joblib.dump(model, \"fake_review_model.pkl\")\n",
    "joblib.dump(vectorizer, \"vectorizer.pkl\")\n",
    "print(\"‚úÖ Model and vectorizer saved successfully.\")\n",
    "\n",
    "# === Step 9: Create Visualizations Folder ===\n",
    "if not os.path.exists(\"screenshots\"):\n",
    "    os.makedirs(\"screenshots\")\n",
    "\n",
    "# === Step 10: Word Cloud ===\n",
    "all_words = ' '.join(df['cleaned_review'])\n",
    "wc = WordCloud(width=800, height=400, background_color='white').generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Most Common Words in Reviews\")\n",
    "plt.savefig(\"screenshots/wordcloud.png\")\n",
    "plt.show()\n",
    "\n",
    "# === Step 11: Review Length Distribution ===\n",
    "df['review_length'] = df['cleaned_review'].apply(lambda x: len(x.split()))\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(df['review_length'], bins=30, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Review Word Count Distribution\")\n",
    "plt.xlabel(\"Word Count\")\n",
    "plt.ylabel(\"Number of Reviews\")\n",
    "plt.savefig(\"screenshots/review_length.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
